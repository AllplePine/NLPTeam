{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gloomy\\software\\-anaconda-\\envs\\gloomytorch\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Downloading readme: 100%|██████████| 10.5k/10.5k [00:00<?, ?B/s]\n",
      "Downloading data: 100%|██████████| 733k/733k [00:01<00:00, 436kB/s]\n",
      "Downloading data: 100%|██████████| 6.36M/6.36M [00:01<00:00, 4.20MB/s]\n",
      "Downloading data: 100%|██████████| 657k/657k [00:00<00:00, 1.27MB/s]\n",
      "Generating test split: 100%|██████████| 4358/4358 [00:00<00:00, 179776.51 examples/s]\n",
      "Generating train split: 100%|██████████| 36718/36718 [00:00<00:00, 1376618.61 examples/s]\n",
      "Generating validation split: 100%|██████████| 3760/3760 [00:00<00:00, 1020617.59 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "datasets = load_dataset('wikitext','wikitext-2-raw-v1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 36718/36718 [00:01<00:00, 34023.36 examples/s]\n",
      "Map: 100%|██████████| 4358/4358 [00:00<00:00, 32080.30 examples/s]\n",
      "Map: 100%|██████████| 3760/3760 [00:00<00:00, 32700.73 examples/s]\n",
      "Filter: 100%|██████████| 36718/36718 [00:00<00:00, 438694.84 examples/s]\n",
      "Filter: 100%|██████████| 4358/4358 [00:00<00:00, 223192.26 examples/s]\n",
      "Filter: 100%|██████████| 3760/3760 [00:00<00:00, 173067.28 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def preprocess_text(sentence):\n",
    "\t# lowering the sentence and storing in text vaiable\n",
    "\ttext = sentence['text'].lower()\n",
    "\t# removing other than characters and punctuations\n",
    "\ttext = re.sub('[^a-z?!.,]', ' ', text)\n",
    "\ttext = re.sub('\\s\\s+', ' ', text) # removing double spaces\n",
    "\tsentence['text'] = text\n",
    "\treturn sentence\n",
    "\n",
    "\n",
    "datasets['train'] = datasets['train'].map(preprocess_text)\n",
    "datasets['test'] = datasets['test'].map(preprocess_text)\n",
    "datasets['validation'] = datasets['validation'].map(preprocess_text)\n",
    "\n",
    "datasets['train'] = datasets['train'].filter(lambda x: len(x['text']) > 20)\n",
    "datasets['test'] = datasets['test'].filter(lambda x: len(x['text']) > 20)\n",
    "datasets['validation'] = datasets['validation'].filter(\n",
    "\tlambda x: len(x['text']) > 20)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\gloomy\\software\\-anaconda-\\envs\\gloomytorch\\lib\\site-packages\\huggingface_hub\\file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "Map: 100%|██████████| 2312/2312 [00:00<00:00, 3599.70 examples/s]\n"
     ]
    }
   ],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "from transformers import DataCollatorWithPadding\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"distilbert-base-uncased-finetuned-sst-2-english\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "\n",
    "# Tokenizer\n",
    "def tokenize(sentence):\n",
    "\tsentence = tokenizer(sentence['text'], truncation=True)\n",
    "\treturn sentence\n",
    "\n",
    "\n",
    "tokenized_inputs = datasets['test'].map(tokenize)\n",
    "tokenized_inputs = tokenized_inputs.remove_columns(['text'])\n",
    "\n",
    "\n",
    "# DataCollator\n",
    "batch = 16\n",
    "data_collator = DataCollatorWithPadding(\n",
    "\ttokenizer=tokenizer, padding=True, return_tensors=\"pt\")\n",
    "dataloader = DataLoader(\n",
    "\ttokenized_inputs, batch_size=batch, collate_fn=data_collator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.fft as fft\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "class PositionalEncoding(torch.nn.Module):\n",
    "\n",
    "\n",
    "\tdef __init__(self, d_model, max_sequence_length):\n",
    "\t\tsuper().__init__()\n",
    "\t\tself.d_model = d_model\n",
    "\t\tself.max_sequence_length = max_sequence_length\n",
    "\t\tself.positional_encoding = self.create_positional_encoding().to(device)\n",
    "\n",
    "\tdef create_positional_encoding(self):\n",
    "\n",
    "\t\t# Initialize positional encoding matrix\n",
    "\t\tpositional_encoding = np.zeros((self.max_sequence_length, self.d_model))\n",
    "\n",
    "\t\t# Calculate positional encoding for each position and each dimension\n",
    "\t\tfor pos in range(self.max_sequence_length):\n",
    "\t\t\tfor i in range(0, self.d_model, 2):\n",
    "\t\t\t\t# Apply sin to even indices in the array; indices in Python start at 0 so i is even.\n",
    "\t\t\t\tpositional_encoding[pos, i] = np.sin(pos / (10000 ** ((2 * i) / self.d_model)))\n",
    "\n",
    "\t\t\t\tif i + 1 < self.d_model:\n",
    "\t\t\t\t\t# Apply cos to odd indices in the array; we add 1 to i because indices in Python start at 0.\n",
    "\t\t\t\t\tpositional_encoding[pos, i + 1] = np.cos(pos / (10000 ** ((2 * i) / self.d_model)))\n",
    "\n",
    "\t\t# Convert numpy array to PyTorch tensor and return it\n",
    "\t\treturn torch.from_numpy(positional_encoding).float()\n",
    "\n",
    "\tdef forward(self, x):\n",
    "\t\texpanded_tensor = torch.unsqueeze(self.positional_encoding, 0).expand(x.size(0), -1, -1).to(device)\n",
    "\n",
    "\t\treturn x.to(device) + expanded_tensor[:,:x.size(1), :]\n",
    "\n",
    "class PositionalEmbedding(nn.Module):\n",
    "    def __init__(self, sequence_length, vocab_size, embed_dim):\n",
    "        super(PositionalEmbedding, self).__init__()\n",
    "        self.token_embeddings = nn.Embedding(vocab_size, embed_dim)\n",
    "        self.position_embeddings = PositionalEncoding(embed_dim,sequence_length)\n",
    "\n",
    "    def forward(self, inputs):\n",
    "        embedded_tokens = self.token_embeddings(inputs).to(device)\n",
    "        embedded_positions = self.position_embeddings(embedded_tokens).to(device)\n",
    "        return embedded_positions.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNetEncoder(nn.Module):\n",
    "\n",
    "    def __init__(self,embed_dim, dense_dim):\n",
    "        super(FNetEncoder,self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.dense_proj = nn.Sequential(nn.Linear(self.embed_dim,self.dense_dim), nn.ReLU(), nn.Linear(self.dense_dim,self.embed_dim))\n",
    "\n",
    "        self.layernorm_1 = nn.LayerNorm(self.embed_dim)\n",
    "        self.layernorm_2 = nn.LayerNorm(self.embed_dim)\n",
    "\n",
    "    def forward(self,inputs):\n",
    "\n",
    "        fft_result = fft.fft2(inputs)\n",
    "\n",
    "        #taking real part\n",
    "        fft_real = fft_result.real.float()\n",
    "\n",
    "        proj_input = self.layernorm_1 (inputs + fft_real)\n",
    "        proj_output = self.dense_proj(proj_input)\n",
    "        return self.layernorm_2(proj_input +proj_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNetDecoder(nn.Module):\n",
    "\n",
    "    def __init__(self,embed_dim,dense_dim,num_heads):\n",
    "        super(FNetDecoder,self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.dense_dim = dense_dim\n",
    "        self.num_heads = num_heads\n",
    "\n",
    "        self.attention_1 = nn.MultiheadAttention(embed_dim,num_heads,batch_first=True)\n",
    "        self.attention_2 = nn.MultiheadAttention(embed_dim,num_heads,batch_first=True)\n",
    "\n",
    "        self.dense_proj = nn.Sequential(nn.Linear(embed_dim, dense_dim),nn.ReLU(),nn.Linear(dense_dim, embed_dim))\n",
    "\n",
    "        self.layernorm_1 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm_2 = nn.LayerNorm(embed_dim)\n",
    "        self.layernorm_3 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, inputs, encoder_outputs, mask=None):\n",
    "        causal_mask = nn.Transformer.generate_square_subsequent_mask(inputs.size(1)).to(device)\n",
    "\n",
    "        attention_output_1, _ = self.attention_1(inputs, inputs, inputs, attn_mask=causal_mask)\n",
    "        out_1 = self.layernorm_1(inputs + attention_output_1)\n",
    "\n",
    "        if mask != None:\n",
    "            attention_output_2, _ = self.attention_2(out_1, encoder_outputs, encoder_outputs, key_padding_mask =torch.transpose(mask, 0, 1).to(device))\n",
    "        else:\n",
    "            attention_output_2, _ = self.attention_2(out_1, encoder_outputs, encoder_outputs)\n",
    "            out_2 = self.layernorm_2(out_1 + attention_output_2)\n",
    "\n",
    "        proj_output = self.dense_proj(out_2)\n",
    "        return self.layernorm_3(out_2 + proj_output)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FNetModel(nn.Module):\n",
    "\tdef __init__(self, max_length, vocab_size, embed_dim, latent_dim, num_heads):\n",
    "\t\tsuper(FNetModel, self).__init__()\n",
    "\n",
    "\t\tself.encoder_inputs = PositionalEmbedding(max_length,vocab_size, embed_dim)\n",
    "\t\tself.encoder1 = FNetEncoder(embed_dim, latent_dim)\n",
    "\t\tself.encoder2 = FNetEncoder(embed_dim, latent_dim)\n",
    "\t\tself.encoder3 = FNetEncoder(embed_dim, latent_dim)\n",
    "\t\tself.encoder4 = FNetEncoder(embed_dim, latent_dim)\n",
    "\n",
    "\n",
    "\t\tself.decoder_inputs = PositionalEmbedding(max_length,vocab_size, embed_dim)\n",
    "\t\tself.decoder1 = FNetDecoder(embed_dim, latent_dim, num_heads)\n",
    "\t\tself.decoder2 = FNetDecoder(embed_dim, latent_dim, num_heads)\n",
    "\t\tself.decoder3 = FNetDecoder(embed_dim, latent_dim, num_heads)\n",
    "\t\tself.decoder4 = FNetDecoder(embed_dim, latent_dim, num_heads)\n",
    "\n",
    "\n",
    "\t\tself.dropout = nn.Dropout(0.5)\n",
    "\t\tself.dense = nn.Linear(embed_dim, vocab_size)\n",
    "\n",
    "\tdef encoder(self,encoder_inputs):\n",
    "\t\tx_encoder = self.encoder_inputs(encoder_inputs)\n",
    "\t\tx_encoder = self.encoder1(x_encoder)\n",
    "\t\tx_encoder = self.encoder2(x_encoder)\n",
    "\t\tx_encoder = self.encoder3(x_encoder)\n",
    "\t\tx_encoder = self.encoder4(x_encoder)\n",
    "\t\treturn x_encoder\n",
    "\n",
    "\tdef decoder(self,decoder_inputs,encoder_output,att_mask):\n",
    "\t\tx_decoder = self.decoder_inputs(decoder_inputs)\n",
    "\t\tx_decoder = self.decoder1(x_decoder, encoder_output,att_mask) ## HERE for inference\n",
    "\t\tx_decoder = self.decoder2(x_decoder, encoder_output,att_mask) ## HERE for inference\n",
    "\t\tx_decoder = self.decoder3(x_decoder, encoder_output,att_mask) ## HERE for inference\n",
    "\t\tx_decoder = self.decoder4(x_decoder, encoder_output,att_mask) ## HERE for inference\n",
    "\t\tdecoder_outputs = self.dense(x_decoder)\n",
    "\n",
    "\t\treturn decoder_outputs\n",
    "\n",
    "\tdef forward(self, encoder_inputs, decoder_inputs,att_mask = None):\n",
    "\t\tencoder_output = self.encoder(encoder_inputs)\n",
    "\t\tdecoder_output = self.decoder(decoder_inputs,encoder_output,att_mask=None)\n",
    "\t\treturn decoder_output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assuming your constants are defined like this:\n",
    "MAX_LENGTH = 512\n",
    "VOCAB_SIZE = len(tokenizer.vocab)\n",
    "EMBED_DIM = 256\n",
    "LATENT_DIM = 100\n",
    "NUM_HEADS = 4\n",
    "\n",
    "# Create an instance of the model\n",
    "fnet_model = FNetModel(MAX_LENGTH, VOCAB_SIZE, EMBED_DIM, LATENT_DIM, NUM_HEADS).to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[13], line 14\u001b[0m\n\u001b[0;32m     12\u001b[0m att_mask \u001b[38;5;241m=\u001b[39m batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m][:,:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device)\u001b[38;5;241m.\u001b[39mto(dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mbool\u001b[39m)\n\u001b[0;32m     13\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 14\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfnet_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mencoder_inputs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdecoder_inputs_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43matt_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m decoder_inputs_tensor\u001b[38;5;241m.\u001b[39mmasked_fill(batch[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mattention_mask\u001b[39m\u001b[38;5;124m'\u001b[39m][:,\u001b[38;5;241m1\u001b[39m:]\u001b[38;5;241m.\u001b[39mne(\u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m100\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m, VOCAB_SIZE), decoder_inputs_tensor\u001b[38;5;241m.\u001b[39mreshape(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\gloomy\\software\\-anaconda-\\envs\\gloomytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gloomy\\software\\-anaconda-\\envs\\gloomytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[9], line 42\u001b[0m, in \u001b[0;36mFNetModel.forward\u001b[1;34m(self, encoder_inputs, decoder_inputs, att_mask)\u001b[0m\n\u001b[0;32m     40\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, encoder_inputs, decoder_inputs,att_mask \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     41\u001b[0m \tencoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mencoder(encoder_inputs)\n\u001b[1;32m---> 42\u001b[0m \tdecoder_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdecoder_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43matt_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     43\u001b[0m \t\u001b[38;5;28;01mreturn\u001b[39;00m decoder_output\n",
      "Cell \u001b[1;32mIn[9], line 35\u001b[0m, in \u001b[0;36mFNetModel.decoder\u001b[1;34m(self, decoder_inputs, encoder_output, att_mask)\u001b[0m\n\u001b[0;32m     33\u001b[0m x_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder2(x_decoder, encoder_output,att_mask) \u001b[38;5;66;03m## HERE for inference\u001b[39;00m\n\u001b[0;32m     34\u001b[0m x_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdecoder3(x_decoder, encoder_output,att_mask) \u001b[38;5;66;03m## HERE for inference\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m x_decoder \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdecoder4\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx_decoder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_output\u001b[49m\u001b[43m,\u001b[49m\u001b[43matt_mask\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m## HERE for inference\u001b[39;00m\n\u001b[0;32m     36\u001b[0m decoder_outputs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense(x_decoder)\n\u001b[0;32m     38\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m decoder_outputs\n",
      "File \u001b[1;32mc:\\Users\\gloomy\\software\\-anaconda-\\envs\\gloomytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gloomy\\software\\-anaconda-\\envs\\gloomytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[8], line 27\u001b[0m, in \u001b[0;36mFNetDecoder.forward\u001b[1;34m(self, inputs, encoder_outputs, mask)\u001b[0m\n\u001b[0;32m     25\u001b[0m     attention_output_2, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mattention_2(out_1, encoder_outputs, encoder_outputs, key_padding_mask \u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mtranspose(mask, \u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m)\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[0;32m     26\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 27\u001b[0m     attention_output_2, _ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattention_2\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout_1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mencoder_outputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     28\u001b[0m     out_2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlayernorm_2(out_1 \u001b[38;5;241m+\u001b[39m attention_output_2)\n\u001b[0;32m     30\u001b[0m proj_output \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdense_proj(out_2)\n",
      "File \u001b[1;32mc:\\Users\\gloomy\\software\\-anaconda-\\envs\\gloomytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\gloomy\\software\\-anaconda-\\envs\\gloomytorch\\lib\\site-packages\\torch\\nn\\modules\\module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\gloomy\\software\\-anaconda-\\envs\\gloomytorch\\lib\\site-packages\\torch\\nn\\modules\\activation.py:1266\u001b[0m, in \u001b[0;36mMultiheadAttention.forward\u001b[1;34m(self, query, key, value, key_padding_mask, need_weights, attn_mask, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   1252\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mmulti_head_attention_forward(\n\u001b[0;32m   1253\u001b[0m         query, key, value, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39membed_dim, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_heads,\n\u001b[0;32m   1254\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_weight, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39min_proj_bias,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1263\u001b[0m         average_attn_weights\u001b[38;5;241m=\u001b[39maverage_attn_weights,\n\u001b[0;32m   1264\u001b[0m         is_causal\u001b[38;5;241m=\u001b[39mis_causal)\n\u001b[0;32m   1265\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1266\u001b[0m     attn_output, attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmulti_head_attention_forward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1267\u001b[0m \u001b[43m        \u001b[49m\u001b[43mquery\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43membed_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1268\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43min_proj_bias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1269\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_k\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias_v\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43madd_zero_attn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1270\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdropout\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mout_proj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1271\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtraining\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1272\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkey_padding_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkey_padding_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1273\u001b[0m \u001b[43m        \u001b[49m\u001b[43mneed_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mneed_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1274\u001b[0m \u001b[43m        \u001b[49m\u001b[43mattn_mask\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mattn_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1275\u001b[0m \u001b[43m        \u001b[49m\u001b[43maverage_attn_weights\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maverage_attn_weights\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1276\u001b[0m \u001b[43m        \u001b[49m\u001b[43mis_causal\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mis_causal\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1277\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_first \u001b[38;5;129;01mand\u001b[39;00m is_batched:\n\u001b[0;32m   1278\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m attn_output\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m0\u001b[39m), attn_output_weights\n",
      "File \u001b[1;32mc:\\Users\\gloomy\\software\\-anaconda-\\envs\\gloomytorch\\lib\\site-packages\\torch\\nn\\functional.py:5469\u001b[0m, in \u001b[0;36mmulti_head_attention_forward\u001b[1;34m(query, key, value, embed_dim_to_check, num_heads, in_proj_weight, in_proj_bias, bias_k, bias_v, add_zero_attn, dropout_p, out_proj_weight, out_proj_bias, training, key_padding_mask, need_weights, attn_mask, use_separate_proj_weight, q_proj_weight, k_proj_weight, v_proj_weight, static_k, static_v, average_attn_weights, is_causal)\u001b[0m\n\u001b[0;32m   5467\u001b[0m     attn_output_weights \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mbaddbmm(attn_mask, q_scaled, k\u001b[38;5;241m.\u001b[39mtranspose(\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m))\n\u001b[0;32m   5468\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 5469\u001b[0m     attn_output_weights \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbmm\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_scaled\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtranspose\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   5470\u001b[0m attn_output_weights \u001b[38;5;241m=\u001b[39m softmax(attn_output_weights, dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[0;32m   5471\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m dropout_p \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0.0\u001b[39m:\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# # Define your optimizer and loss function\n",
    "optimizer = torch.optim.Adam(fnet_model.parameters())\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=0)\n",
    "\n",
    "epochs = 3\n",
    "for epoch in range(epochs):\n",
    "\ttrain_loss = 0\n",
    "\tfor batch in dataloader:\n",
    "\t\tencoder_inputs_tensor = batch['input_ids'][:,:-1].to(device)\n",
    "\t\tdecoder_inputs_tensor = batch['input_ids'][:,1:].to(device)\n",
    "\n",
    "\t\tatt_mask = batch['attention_mask'][:,:-1].to(device).to(dtype=bool)\n",
    "\t\toptimizer.zero_grad()\n",
    "\t\toutputs = fnet_model(encoder_inputs_tensor, decoder_inputs_tensor,att_mask)\n",
    "\t\tdecoder_inputs_tensor.masked_fill(batch['attention_mask'][:,1:].ne(1).to(device), -100).to(device)\n",
    "\n",
    "\t\tloss = criterion(outputs.reshape(-1, VOCAB_SIZE), decoder_inputs_tensor.reshape(-1))\n",
    "\t\ttrain_loss = train_loss + loss.item()\n",
    "\t\tloss.backward()\n",
    "\t\toptimizer.step()\n",
    "\tprint (f\" epoch: {epoch}, train_loss : {train_loss}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_LENGTH =100 # your MAX_LENGTH value\n",
    " \n",
    "def decode_sentence(input_sentence, fnet_model):\n",
    "    fnet_model.eval()\n",
    " \n",
    "    with torch.no_grad():\n",
    "        tokenized_input_sentence = torch.tensor(tokenizer(preprocess_text(input_sentence)['text'])['input_ids']).to(device)#\n",
    "        tokenzied_target_sentence = torch.tensor([101]).to(device) # '[CLS]' token\n",
    "        current_text = preprocess_text(input_sentence)['text']\n",
    "        for i in range(MAX_LENGTH):\n",
    "            predictions = fnet_model(tokenized_input_sentence[:-1].unsqueeze(0),tokenzied_target_sentence.unsqueeze(0))\n",
    "            predicted_index = torch.argmax(predictions[0, -1, :]).item()\n",
    "            predicted_token = tokenizer.decode(predicted_index)\n",
    "            if predicted_token == \"[SEP]\":  # Assuming [end] is the end token\n",
    "              break\n",
    "            current_text += \" \"+ predicted_token\n",
    "            tokenized_target_sentence = torch.cat([tokenzied_target_sentence, torch.tensor([predicted_index]).to(device)], 0).to(device)\n",
    "            tokenized_input_sentence = torch.tensor(tokenizer(current_text)['input_ids']).to(device)\n",
    "        return current_text\n",
    "decode_sentence({'text': 'How are you ?'}, fnet_model)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gloomytorch",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
